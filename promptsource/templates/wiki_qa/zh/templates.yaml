dataset: wiki_qa
subset: zh
templates:
  148e8e91-4f38-4427-8806-8a407268cda9: !Template
    answer_choices: "\u4E0D ||| \u662F\u7684"
    id: 148e8e91-4f38-4427-8806-8a407268cda9
    jinja: "\u95EE\u9898\uFF1A {{question}}?\n\"{{answer}}\" \u4F1A\u662F\u4E00\u4E2A\
      \u5408\u7406\u7684\u7B54\u6848\u5417\uFF1F |||\n{{ answer_choices[label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Is This True?
    reference: Model is inserted both question and answer and output whether the answer
      is correct or not.
  2395d5ce-5abd-4193-9cf1-863c7271a4f0: !Template
    answer_choices: "\u4E0D ||| \u662F\u7684"
    id: 2395d5ce-5abd-4193-9cf1-863c7271a4f0
    jinja: "\u6211\u6B63\u5728\u9A8C\u8BC1\u81EA\u52A8\u7CFB\u7EDF\u751F\u6210\u7684\
      \u4EE5\u4E0B\u95EE\u9898\u7684\u7B54\u6848\uFF1A {{question}}\n\u5EFA\u8BAE\u7B54\
      \u6848\uFF1A {{answer}}\n\u6211\u5E94\u8BE5\u9A8C\u8BC1\u8FD9\u4E2A\u7B54\u6848\
      \u5417\uFF1F\n|||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: automatic_system
    reference: ''
  3480df1e-88bb-4b3d-90df-3f292463eb76: !Template
    answer_choices: null
    id: 3480df1e-88bb-4b3d-90df-3f292463eb76
    jinja: "{% if label == 1 %}\n\"{{answer}}\" \u7684\u95EE\u9898\u662F\u4EC0\u4E48\
      \uFF1F\u4E3B\u9898\u662F{{document_title}}\u3002|||\n\"{{question}}?\"\n{% endif\
      \ %}\n"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: false
    name: Jeopardy style
    reference: Given a passage (an answer), generate the question.
  8a9f2146-aa30-4e17-b1e2-aeb858b08b55: !Template
    answer_choices: null
    id: 8a9f2146-aa30-4e17-b1e2-aeb858b08b55
    jinja: "{% if label == 1 %}\n\u786E\u5B9A\u95EE\u7B54\u5BF9\u7684\u4E3B\u9898\u3002\
      \n\u95EE\u9898\uFF1A \"{{question}}?\";  \u56DE\u7B54\uFF1A \"{{answer}}\"?\
      \ \u8BDD\u9898\uFF1A |||\n{{document_title}}\n{% endif %}\n"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: false
    name: Topic Prediction - Question and Answer Pair
    reference: Given a correct Question-Answer pair, generate the topic.
  a99a68fa-46ae-4331-8b97-fcf751db3f6f: !Template
    answer_choices: null
    id: a99a68fa-46ae-4331-8b97-fcf751db3f6f
    jinja: "{% if label == 1 %}\n\u751F\u6210\u4E00\u4E2A\u5173\u4E8E\u4E3B\u9898\u201C\
      {{document_title}}\u201D\u7684\u95EE\u9898\uFF0C\u5176\u7B54\u6848\u662F\uFF1A\
      \ {{answer}}.|||\n{{question}}?\n{% endif %}\n"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: false
    name: Generate Question from Topic
    reference: Given a topic, generate a question.
  add469e1-b8d9-4926-8f38-3a60c85a7d2b: !Template
    answer_choices: "\u4E0D ||| \u662F\u7684"
    id: add469e1-b8d9-4926-8f38-3a60c85a7d2b
    jinja: "\u95EE\u9898\uFF1A {{question}}\n\u6211\u5728\u8C37\u6B4C\u4E0A\u627E\u5230\
      \u4E86\u4EE5\u4E0B\u7B54\u6848\uFF1A {{answer}}\n\u8FD9\u662F\u4E00\u4E2A\u6B63\
      \u786E\u7684\u7B54\u6848\u5417\uFF1F\u662F\u8FD8\u662F\u4E0D\u662F\u3002\n|||\n\
      {{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: found_on_google
    reference: ''
  b0ad07f8-8799-4dd8-8f55-82f3f817f1fd: !Template
    answer_choices: null
    id: b0ad07f8-8799-4dd8-8f55-82f3f817f1fd
    jinja: "{% if label == 1 %}\n\u786E\u5B9A\u95EE\u9898\u7684\u4E3B\u9898\u3002\n\
      \u95EE\u9898\uFF1A \"{{question}}?\"\n\u8BDD\u9898\uFF1A |||\n{{document_title}}\n\
      {% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: false
    name: Topic Prediction - Question Only
    reference: Given a Question, generate the topic.
  bfa3adac-d75b-4e09-aa92-dc38e334937f: !Template
    answer_choices: "\u9519\u8BEF\u7684 ||| \u771F\u7684"
    id: bfa3adac-d75b-4e09-aa92-dc38e334937f
    jinja: "\u7EC3\u4E60\u662F\u51B3\u5B9A\u95EE\u9898\u662F\u5426\u63A5\u53D7\u63D0\
      \u51FA\u7684\u5EFA\u8BAE\u4F5C\u4E3A\u6B63\u786E\u7B54\u6848\u3002\u5982\u679C\
      \u662F\uFF0C\u5199\"{{answer_choices[1]}}\"\uFF0C\u5426\u5219\u5199\"{{answer_choices[0]}}\"\
      \u3002\nQuestion: {{question}}\nSuggestion: {{answer}}\n|||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: exercise
    reference: ''
  c802cf19-59a7-4a3e-a6ab-5cbb1f169c70: !Template
    answer_choices: "\u4E0D ||| \u662F\u7684"
    id: c802cf19-59a7-4a3e-a6ab-5cbb1f169c70
    jinja: "\u8FD9\u662F\u5BF9\u4EE5\u4E0B\u5173\u4E8E {{document_title}} \u7684\u95EE\
      \u9898\u7684\u6B63\u786E\u7B54\u6848\u3002\u662F\u8FD8\u662F\u4E0D\u662F\uFF1F\
      \n\u56DE\u7B54\uFF1A {{answer}}\n\u95EE\u9898\uFF1A {{question}}\n|||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: Decide_good_answer
    reference: ''
  cdc54124-723e-4e1c-878c-aeaabf55c28c: !Template
    answer_choices: null
    id: cdc54124-723e-4e1c-878c-aeaabf55c28c
    jinja: "{% if label == 1 %}\n\u786E\u5B9A\u6587\u7AE0\u7684\u4E3B\u9898\u3002\n\
      \"{{answer}}\"\n\u8BDD\u9898\uFF1A|||\n{{document_title}}\n{% endif %}\n"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: false
    name: Topic Prediction - Answer Only
    reference: Given a correct Answer (as a text passage), generate the topic.
  d827a178-ff54-4bbf-bc6d-8756950ae5c5: !Template
    answer_choices: null
    id: d827a178-ff54-4bbf-bc6d-8756950ae5c5
    jinja: "{% if label == 1 %}\n\u56DE\u7B54\u8FD9\u4E2A\u95EE\u9898\uFF1A {{question}}?|||\n\
      {{answer}}\n{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: true
    name: Direct Answer to Question
    reference: Generates an answers given a question.
