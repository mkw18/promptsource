dataset: imdb
subset: ko
templates:
  02ff2949-0f45-4d97-941e-6fa4c0afbc2d: !Template
    answer_choices: "\uBD80\uC815\uC801\uC778 ||| \uAE0D\uC815\uC801\uC778"
    id: 02ff2949-0f45-4d97-941e-6fa4c0afbc2d
    jinja: "\uB2E4\uC74C \uC601\uD654 \uB9AC\uBDF0\uB294 \uC5B4\uB5A4 \uAC10\uC815\
      \uC744 \uD45C\uD604\uD569\uB2C8\uAE4C? {{text}} ||| {{ answer_choices [label]\
      \ }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Movie Expressed Sentiment 2
    reference: ''
  2351d12a-e630-4d19-8b41-e199266e38f7: !Template
    answer_choices: "\uB098\uC05C ||| \uC88B\uC740"
    id: 2351d12a-e630-4d19-8b41-e199266e38f7
    jinja: "{{text}} \uB9AC\uBDF0\uC5B4\uAC00 \uC774 \uC601\uD654 {{\"\uC88B\uAC70\
      \uB098 \uB098\uC058\uAC70\uB098\"}}\uB97C \uCC3E\uC558\uC2B5\uB2C8\uAE4C? |||\
      \ {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: Reviewer Opinion bad good choices
    reference: ''
  5f372fb1-795a-47b6-8ddf-c4fd1579e76a: !Template
    answer_choices: "\uBD80\uC815\uC801\uC778 ||| \uAE0D\uC815\uC801\uC778"
    id: 5f372fb1-795a-47b6-8ddf-c4fd1579e76a
    jinja: "{{text}} \n\uC774 \uB9AC\uBDF0\uAC00 {{\"\uAE0D\uC815\uC801\uC774\uB4E0\
      \ \uBD80\uC815\uC801\uC774\uB4E0\"}}\uC785\uB2C8\uAE4C? ||| \n{{answer_choices[label]\
      \ }}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: 'Sentiment with choices '
    reference: ''
  866474a5-1498-46b7-bfee-ac0c5160707f: !Template
    answer_choices: "\uBD80\uC815\uC801\uC778 ||| \uAE0D\uC815\uC801\uC778"
    id: 866474a5-1498-46b7-bfee-ac0c5160707f
    jinja: "{{text}} \uAD00\uAC1D\uC740 \uC601\uD654\uC5D0 \uB300\uD574 \uC5B4\uB5BB\
      \uAC8C \uB290\uB07C\uB294\uAC00? ||| {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Reviewer Sentiment Feeling
    reference: ''
  96538f30-f2c1-430e-8fc6-936a16966d9c: !Template
    answer_choices: "\uBD80\uC815\uC801\uC778 ||| \uAE0D\uC815\uC801\uC778"
    id: 96538f30-f2c1-430e-8fc6-936a16966d9c
    jinja: "{{text}} \uC791\uAC00\uB294 \uC601\uD654\uC5D0 \uB300\uD574 \uC5B4\uB5A4\
      \ \uAC10\uC815\uC744 \uD45C\uD604\uD558\uB294\uAC00? ||| {{ answer_choices [label]\
      \ }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Writer Expressed Sentiment
    reference: ''
  af51297c-38a3-4d6c-a8b5-04b1243d7443: !Template
    answer_choices: "\uBD80\uC815\uC801\uC778 ||| \uAE0D\uC815\uC801\uC778"
    id: af51297c-38a3-4d6c-a8b5-04b1243d7443
    jinja: "{{text}} \uC601\uD654\uC5D0 \uD45C\uD604\uB41C \uAC10\uC131\uC740 |||\
      \ {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Movie Expressed Sentiment
    reference: ''
  b93b74ac-fe95-40b4-9610-318b46ab820f: !Template
    answer_choices: "\uBD80\uC815\uC801\uC778 ||| \uAE0D\uC815\uC801\uC778"
    id: b93b74ac-fe95-40b4-9610-318b46ab820f
    jinja: "{{text}} \uC774 \uD14D\uC2A4\uD2B8\uC5D0 \uD45C\uD604\uB41C \uAC10\uC815\
      \uC740 \uBB34\uC5C7\uC785\uB2C8\uAE4C? ||| {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Text Expressed Sentiment
    reference: ''
  b9b5d79d-f0b3-4bec-a724-f585db3e93ff: !Template
    answer_choices: "\uBD80\uC815\uC801\uC778 ||| \uAE0D\uC815\uC801\uC778"
    id: b9b5d79d-f0b3-4bec-a724-f585db3e93ff
    jinja: "{{text}} \uC774\uAC83\uC740 \uD655\uC2E4\uD788 ||| {{ answer_choices [1-label]}}\
      \ \uB9AC\uBDF0\uAC00 \uC544\uB2D9\uB2C8\uB2E4."
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: false
    name: Negation template for positive and negative
    reference: ''
  bd82ba0f-01d4-4fa1-bf8d-07e392c00cd9: !Template
    answer_choices: "\uC544\uB2C8 ||| \uC608"
    id: bd82ba0f-01d4-4fa1-bf8d-07e392c00cd9
    jinja: "{{text}} \uB9AC\uBDF0\uC5B4\uAC00 \uC601\uD654\uB97C \uC7AC\uBBF8\uC788\
      \uAC8C \uBD24\uB098\uC694? ||| {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Reviewer Enjoyment Yes No
    reference: ''
  c70d1687-2421-49a2-9553-91b8bac4cfbe: !Template
    answer_choices: "\uBD80\uC815\uC801\uC778 ||| \uAE0D\uC815\uC801\uC778"
    id: c70d1687-2421-49a2-9553-91b8bac4cfbe
    jinja: "{{text}} \uC601\uD654\uC5D0 \uB300\uD55C \uD3C9\uB860\uAC00\uC758 \uAC10\
      \uC815\uC740 \uBB34\uC5C7\uC785\uB2C8\uAE4C? ||| {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Reviewer Expressed Sentiment
    reference: ''
  e01970ab-42c0-4e6e-a08f-4940d889ef37: !Template
    answer_choices: "\uADF8\uB4E4\uC740 \uADF8\uAC83\uC744 \uC88B\uC544\uD558\uC9C0\
      \ \uC54A\uC558\uB2E4! ||| \uADF8\uB4E4\uC740 \uADF8\uAC83\uC744 \uC88B\uC544\
      \uD588\uB2E4"
    id: e01970ab-42c0-4e6e-a08f-4940d889ef37
    jinja: "{{text}} \uD3C9\uB860\uAC00\uB294 \uC601\uD654\uC5D0 \uB300\uD574 \uC5B4\
      \uB5BB\uAC8C \uC0DD\uAC01\uD569\uB2C8\uAE4C? ||| {{ answer_choices [label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Reviewer Enjoyment
    reference: ''
