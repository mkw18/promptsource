dataset: piqa
subset: zh
templates:
  16e97a16-c958-4956-bfba-279f88dafd5b: !Template
    answer_choices: '{{sol1}} ||| {{sol2}}'
    id: 16e97a16-c958-4956-bfba-279f88dafd5b
    jinja: "\u76EE\u6807\uFF1A {{goal}}\n\n\u54EA\u4E2A\u662F\u6B63\u786E\u7684\u7ED3\
      \u5C40\uFF1F\n- {{sol1}}\n- {{sol2}}\n\n\u56DE\u7B54\uFF1A\n|||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: what_is_the_correct_ending
    reference: ''
  3f336295-c1f7-410a-8fc6-d2ed79487aa4: !Template
    answer_choices: '{{sol1}} ||| {{sol2}}'
    id: 3f336295-c1f7-410a-8fc6-d2ed79487aa4
    jinja: "{{\"\u89E3\u51B3\u65B9\u68481\"}}: {{sol1}}\n{{\"\u89E3\u51B3\u65B9\u6848\
      2\"}}: {{sol2}}\n\n\u76EE\u6807\uFF1A {{goal}}\n\n\u9274\u4E8E\u76EE\u6807\uFF0C\
      \u6B63\u786E\u7684\u89E3\u51B3\u65B9\u6848\u662F\u4EC0\u4E48\uFF1F\n\n\u901A\
      \u8FC7\u590D\u5236\u6B63\u786E\u7684\u89E3\u51B3\u65B9\u6848\u6765\u56DE\u7B54\
      \n|||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: pick_correct_choice_with_choice_given_before_goal
    reference: ''
  44778818-7b73-4262-a294-c00fc32b6c2c: !Template
    answer_choices: 1 ||| 2
    id: 44778818-7b73-4262-a294-c00fc32b6c2c
    jinja: "\u53E5\u5B50\uFF1A {{goal}}\n\n\u9009\u62E9 {{answer_choices[0]}}: {{sol1}}\n\
      \n\u9009\u62E9 {{answer_choices[1]}}: {{sol2}}\n\n\u53E5\u5B50\u7ED3\u675F\u7684\
      \u6B63\u786E\u9009\u62E9\u7684\u7D22\u5F15\u662F\u4EC0\u4E48\uFF1F\n\n\u56DE\
      \u7B54\uFF1A\n\n|||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: pick_correct_choice_index
    reference: ''
  5f4b4645-9438-4375-9062-083130e6d04e: !Template
    answer_choices: null
    id: 5f4b4645-9438-4375-9062-083130e6d04e
    jinja: "\u7ED9\u5B9A\u76EE\u6807\u548C\u9519\u8BEF\u7684\u89E3\u51B3\u65B9\u6848\
      \uFF0C\u5C06\u5176\u91CD\u5199\u4EE5\u63D0\u4F9B\u6B63\u786E\u7684\u89E3\u51B3\
      \u65B9\u6848\u3002\n\u76EE\u6807\uFF1A {{goal}} \n\u89E3\u51B3\u65B9\u6848\uFF1A\
      \ {{[sol1, sol2][1 - label]}}\n\u66F4\u6B63\u7684\u89E3\u51B3\u65B9\u6848\uFF1A\
      \n|||\n{{[sol1, sol2][label]}}\n"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: false
    name: Correct the solution
    reference: ''
  94c39589-7bfb-4c09-9337-672369459545: !Template
    answer_choices: '{{sol1}} ||| {{sol2}}'
    id: 94c39589-7bfb-4c09-9337-672369459545
    jinja: "\u7528\u6700\u4F73\u9009\u62E9\u5B8C\u6210\u4EE5\u4E0B\u53E5\u5B50\uFF1A\
      \ {{goal}}\n\n\u9009\u62E9\uFF1A\n- {{sol1}}\n- {{sol2}}\n\n\u56DE\u7B54\uFF1A\
      \n\n|||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: finish_sentence_with_correct_choice
    reference: ''
  99565244-4eaf-4004-a28b-4362ba5bcac3: !Template
    answer_choices: "\u4E0D ||| \u662F\u7684"
    id: 99565244-4eaf-4004-a28b-4362ba5bcac3
    jinja: "{{goal}} {{sol2}}\n\u8FD9\u4E2A\u77ED\u8BED\u6709\u610F\u4E49\u5417\uFF1F\
      \n|||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: false
    name: Does this solution make sense? sol2
    reference: ''
  adfef248-f856-44fa-affd-e3223718854e: !Template
    answer_choices: "\u89E3\u51B3\u65B9\u68481 ||| \u89E3\u51B3\u65B9\u68482"
    id: adfef248-f856-44fa-affd-e3223718854e
    jinja: "\u7ED9\u5B9A\u76EE\u6807\u548C2\u4E2A\u89E3\u51B3\u65B9\u6848\uFF0C\u8BF7\
      \u9009\u62E9\u6700\u5408\u9002\u7684\u89E3\u51B3\u65B9\u6848\u3002\n\u76EE\u6807\
      \uFF1A {{goal}}\n- {{\"\u89E3\u51B3\u65B9\u68481\"}}: {{sol1}}\n- {{\"\u89E3\
      \u51B3\u65B9\u68482\"}}: {{sol2}}\n\n\u901A\u8FC7\u9000\u8D27 {{\"\u89E3\u51B3\
      \u65B9\u68481\"}} \u6216\u8005 {{\"\u89E3\u51B3\u65B9\u68482\"}}\n|||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: choose the most appropriate solution
    reference: ''
  b5c69473-eedb-4c4f-a5fa-d4e266e43535: !Template
    answer_choices: null
    id: b5c69473-eedb-4c4f-a5fa-d4e266e43535
    jinja: "Given a sentence, correct it if it doesn't make sense. If it makes sense,\
      \ just return it as the answer.\n\u8F93\u5165\uFF1A {{goal}} {{sol2[0].lower()\
      \ + sol2[1:]}}\n\u8F93\u51FA\uFF1A\n|||\n{{goal}} {{[sol1[0].lower() + sol1[1:],\
      \ sol2[0].lower() + sol2[1:]][label]}}\n"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: false
    name: 'Correct the solution if false: from sol 2'
    reference: ''
  c8c45ef1-2ffc-43d7-8710-b98c2fc4f699: !Template
    answer_choices: null
    id: c8c45ef1-2ffc-43d7-8710-b98c2fc4f699
    jinja: '{{goal}}

      |||

      {{[sol1[0].lower() + sol1[1:], sol2[0].lower() + sol2[1:]][label]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: false
    name: no prompt needed
    reference: ''
  f044def7-01c2-42de-b6ad-4e8c63ab2bf1: !Template
    answer_choices: "\u662F\u7684 ||| \u4E0D"
    id: f044def7-01c2-42de-b6ad-4e8c63ab2bf1
    jinja: "\u8FD9\u4E2A\u77ED\u8BED\u6709\u610F\u4E49\u5417\uFF1F\n{{goal}} {{sol1[0].lower()\
      \ + sol1[1:]}}\n\u56DE\u7B54 {{answer_choices[0]}} \u6216\u8005 {{answer_choices[1]}}\n\
      |||\n{{answer_choices[label]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: Does this solution make sense? sol1
    reference: ''
  f42cd457-a14b-465a-a139-d7d2407a3bac: !Template
    answer_choices: null
    id: f42cd457-a14b-465a-a139-d7d2407a3bac
    jinja: "\u53E5\u5B50\uFF1A {{goal}} {{sol1[0].lower() + sol1[1:]}}\n\u5982\u679C\
      \u53E5\u5B50\u6CA1\u6709\u610F\u4E49\uFF0C\u8BF7\u7EA0\u6B63\u5B83\uFF0C\u4EE5\
      \u4F7F\u5176\u786E\u5B9E\u6709\u610F\u4E49\u3002 \u5426\u5219\uFF0C\u53EA\u9700\
      \u590D\u5236\u5B83\u5373\u53EF\u3002\n\u56DE\u7B54\uFF1A\n|||\n{{goal}} {{[sol1[0].lower()\
      \ + sol1[1:], sol2[0].lower() + sol2[1:]][label]}}\n"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - BLEU
      - ROUGE
      original_task: false
    name: 'Correct the solution if false: from sol 1'
    reference: ''
