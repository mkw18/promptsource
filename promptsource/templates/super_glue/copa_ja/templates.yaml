dataset: super_glue
subset: copa_ja
templates:
  0edd8660-f299-4819-a5ac-633c11177228: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: 0edd8660-f299-4819-a5ac-633c11177228
    jinja: "\u30A8\u30AF\u30B5\u30B5\u30A4\u30BA\uFF1A \u6700\u3082\u3082\u3063\u3068\
      \u3082\u3089\u3057\u3044\u4EE3\u66FF\u54C1\u3092\u9078\u629E\u3057\u3066\u304F\
      \u3060\u3055\u3044\u3002\n\n{{ premise }} {% if question == \"cause\" %} \u306A\
      \u305C\u306A\u3089... {% else %} \u305D\u308C\u3067... {% endif %}\n- {{choice1}}\n\
      - {{choice2}} ||| {% if label != -1 %}{{ answer_choices[label] }}{%endif%}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: exercise
    reference: ''
  150789fe-e309-47a1-82c9-0a4dc2c6b12b: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: 150789fe-e309-47a1-82c9-0a4dc2c6b12b
    jinja: "{% if question == \"effect\" %} \n{{ premise }} \u6B21\u306B\u4F55\u304C\
      \u8D77\u3053\u308B\u304B\u3001\"{{ answer_choices[0] }}\"\u307E\u305F\u306F\"\
      {{ answer_choices[1] }}\"\uFF1F ||| {% if label != -1 %}{{ answer_choices[label]\
      \ }}{%endif%}\n{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: "\u2026What could happen next, C1 or C2?"
    reference: ''
  4d879cbe-2fd7-424a-9d78-3f5200313fba: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: 4d879cbe-2fd7-424a-9d78-3f5200313fba
    jinja: "{{ premise }} \n\n\u79C1\u306F2\u3064\u306E\u30AA\u30D7\u30B7\u30E7\u30F3\
      \u306E\u9593\u3067he\u3057\u3066\u3044\u307E\u3059\u3002 \u3088\u308A\u53EF\u80FD\
      \u6027\u306E\u9AD8\u3044\u3082\u306E\u3092\u9078\u3076\u306E\u3092\u624B\u4F1D\
      \u3063\u3066\u304F\u3060\u3055\u3044 {% if question == \"cause\" %} \u539F\u56E0\
      \uFF1A {% else %} \u52B9\u679C\uFF1A {% endif %}\n- {{choice1}}\n- {{choice2}}\
      \ ||| {% if label != -1 %}{{ answer_choices[label] }}{%endif%}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: i_am_hesitating
    reference: ''
  66ea075e-4d03-4a78-b1fa-9a5228cf0c9d: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: 66ea075e-4d03-4a78-b1fa-9a5228cf0c9d
    jinja: "{{ premise }} {% if question == \"cause\" %} \u3053\u308C\u306F\u8D77\u3053\
      \u3063\u305F\u304B\u3089\u3067\u3059... {% else %} \u7D50\u679C\u3068\u3057\u3066\
      ... {% endif %}\n\u3082\u3063\u3068\u3082\u3089\u3057\u3044\u30AA\u30D7\u30B7\
      \u30E7\u30F3\u3092\u9078\u629E\u3059\u308B\u306E\u3092\u624B\u4F1D\u3063\u3066\
      \u304F\u3060\u3055\u3044\uFF1A\n- {{choice1}}\n- {{choice2}} ||| {% if label\
      \ != -1 %}{{ answer_choices[label] }}{%endif%}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: plausible_alternatives
    reference: ''
  744047dc-1298-45a2-8d68-d67e3f834ded: !Template
    answer_choices: '{{choice1 }} ||| {{choice2}}'
    id: 744047dc-1298-45a2-8d68-d67e3f834ded
    jinja: "\"{{ answer_choices[0] }}\"\u307E\u305F\u306F\"{{ answer_choices[1] }}\"\
      \uFF1F {{ premise }} {% if question == \"cause\" %} \u306A\u305C\u306A\u3089\
      \ {% else %} \u305D\u308C\u3067 {% endif %} ||| {% if label != -1 %}{{ answer_choices[label]\
      \ }}{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: "C1 or C2? premise, so/because\u2026"
    reference: "Adapted from Perez et al. 2021 and Schick & Sch\xFCtz 2021."
  84da62c2-9440-4cfc-bdd4-d70c65e33a82: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: 84da62c2-9440-4cfc-bdd4-d70c65e33a82
    jinja: "{% if question == \"effect\" %} \n{{ premise }} \u305D\u306E\u7D50\u679C\
      \u3001\"{{ answer_choices[0] }}\"\u307E\u305F\u306F\"{{ answer_choices[1] }}\"\
      \uFF1F ||| {% if label != -1 %}{{ answer_choices[label] }}{%endif%}\n{% endif\
      \ %}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: "\u2026As a result, C1 or C2?"
    reference: ''
  8ce80f8a-239e-4393-892c-f63dbb0d9929: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: 8ce80f8a-239e-4393-892c-f63dbb0d9929
    jinja: "{{ premise }} \n\n\u6700\u826F\u306E\u9078\u629E\u80A2\u306F\u4F55\u3067\
      \u3059\u304B\uFF1F\n- {{choice1}}\n- {{choice2}}\n\n\u79C1\u305F\u3061\u306F\
      \u301C\u3092\u63A2\u3057\u3066\u3044\u308B {% if question == \"cause\" %} \u539F\
      \u56E0 {% else %} \u52B9\u679C {% endif %}\n||| {% if label != -1 %}{{answer_choices[label]}}{%endif%}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: best_option
    reference: ''
  8cf2ba73-aee5-4651-b5d4-b1b88afe4abb: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: 8cf2ba73-aee5-4651-b5d4-b1b88afe4abb
    jinja: "{% if question == \"cause\" %} \n{{ premise }} \"{{ answer_choices[0]\
      \ }}\"\u307E\u305F\u306F\"{{ answer_choices[1] }}\"\u306B\u3088\u3063\u3066\u5F15\
      \u304D\u8D77\u3053\u3055\u308C\u308B\u53EF\u80FD\u6027\u304C\u3042\u308A\u307E\
      \u3059\u304B\uFF1F ||| {% if label != -1 %}{{ answer_choices[label] }}{%endif%}\n\
      {% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: "\u2026which may be caused by"
    reference: ''
  a1f9951e-2b6b-4530-9636-9cdf4c1658c5: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: a1f9951e-2b6b-4530-9636-9cdf4c1658c5
    jinja: "\u6B21\u306E\u6587\u306E\u53EF\u80FD\u6027\u306E\u9AD8\u3044\u7D99\u7D9A\
      \u3092\u9078\u629E\u3057\u3066\u304F\u3060\u3055\u3044\u3002\n{{ premise }}\
      \ {% if question == \"cause\" %} \u7D50\u679C\u3068\u3057\u3066\uFF1A {% else\
      \ %} \u7D50\u679C\u3068\u3057\u3066\uFF1A {% endif %}\n- {{choice1}}\n- {{choice2}}\
      \ ||| {% if label != -1 %}{{ answer_choices[label] }}{%endif%}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: more likely
    reference: ''
  a61d8c21-da25-47bf-b5fe-14a8edd650af: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: a61d8c21-da25-47bf-b5fe-14a8edd650af
    jinja: "{{ premise }}\n\n\u3082\u3063\u3068\u3082\u3089\u3057\u3044\u3082\u306E\
      \u3092\u9078\u629E\u3057\u307E\u3059 {% if question == \"cause\" %} \u539F\u56E0\
      \uFF1A {% else %} \u52B9\u679C\uFF1A {% endif %}\n- {{choice1}}\n- {{choice2}}\
      \ ||| {% if label != -1 %}{{ answer_choices[label] }}{%endif%}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: cause_effect
    reference: ''
  a8bf11c3-bea2-45ba-a533-957d8bee5e2e: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: a8bf11c3-bea2-45ba-a533-957d8bee5e2e
    jinja: "{% if question == \"cause\" %} \n{{ premise }} \u306A\u3093\u3067\uFF1F\
      \ \"{{ answer_choices[0] }}\"\u307E\u305F\u306F\"{{ answer_choices[1] }}\"\uFF1F\
      \ ||| {% if label != -1 %}{{ answer_choices[label] }}{%endif%}\n{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: "\u2026why? C1 or C2"
    reference: ''
  f32348cd-d3cb-4619-87b9-e24f99c78567: !Template
    answer_choices: '{{choice1}} ||| {{choice2}}'
    id: f32348cd-d3cb-4619-87b9-e24f99c78567
    jinja: "{{ premise }} {% if question == \"cause\" %} \u306A\u305C\u306A\u3089\
      ... {% else %} \u305D\u308C\u3067... {% endif %}\n\u306E\u4E2D\u304B\u3089\u9078\
      \u3076\uFF1A\n- {{choice1}}\n- {{choice2}} ||| {% if label != -1 %}{{ answer_choices[label]\
      \ }}{%endif%}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: choose
    reference: ''
