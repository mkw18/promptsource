dataset: quarel
subset: fa
templates:
  5904fd73-b1ee-4f89-b7bc-b0fe8cc07c66: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 5904fd73-b1ee-4f89-b7bc-b0fe8cc07c66
    jinja: "\u0633\u0648\u0627\u0644: {{question}}\n\n\u0628\u0631\u0627\u06CC \u067E\
      \u0627\u0633\u062E \u0628\u0647 \u0627\u06CC\u0646 \u0633\u0648\u0627\u0644\
      \ \u0627\u0632 {{\"A\"}} \u0648 {{\"B\"}} \u0627\u0633\u062A\u0641\u0627\u062F\
      \u0647 \u0646\u06A9\u0646\u06CC\u062F \u060C \u0627\u0645\u0627 \u062F\u0631\
      \ \u0639\u0648\u0636 \u060C \u0628\u06CC\u0646 \"{{answer_choices[0]}}\" \u0648\
      \ \"{{answer_choices[1]}}\" \u0631\u0627 \u0627\u0646\u062A\u062E\u0627\u0628\
      \ \u06A9\u0646\u06CC\u062F.\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: do_not_use
    reference: ''
  5b5f9d29-0ad5-4bb9-831a-11fcb115c10d: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 5b5f9d29-0ad5-4bb9-831a-11fcb115c10d
    jinja: "\u062F\u0631 \u0627\u06CC\u0646\u062C\u0627 \u06CC\u06A9 \u0622\u0632\u0645\
      \u0648\u0646 \u0645\u0646\u0637\u0642\u06CC \u0627\u0633\u062A: {{question}}\n\
      \n\u067E\u0627\u0633\u062E \u0628\u06CC\u0646 \"{{answer_choices[0]}}\" \u0648\
      \ \"{{answer_choices[1]}}\" \u0631\u0627 \u0627\u0646\u062A\u062E\u0627\u0628 \u06A9\u0646\u06CC\u062F\
      .\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: logic_test
    reference: ''
  63c58389-605a-42b9-85a6-a2586a954a92: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 63c58389-605a-42b9-85a6-a2586a954a92
    jinja: "\u062F\u0631 \u0627\u06CC\u0646\u062C\u0627 \u06CC\u06A9 \u062F\u0627\u0633\
      \u062A\u0627\u0646 \u06A9\u0648\u062A\u0627\u0647 \u0648\u062C\u0648\u062F \u062F\
      \u0627\u0631\u062F: {{question}}.\n\n\u062D\u0633\u0627\u0633 \u062A\u0631\u06CC\
      \u0646 \u067E\u0627\u0633\u062E \u0628\u06CC\u0646 \"{{answer_choices[0]}}\"\
      \ \u0648 \"{{answer_choices[1]}}\" \u0686\u06CC\u0633\u062A\u061F\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: heres_a_story
    reference: ''
  73a7adbb-41b1-4b4d-b378-d7e17d030a6f: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 73a7adbb-41b1-4b4d-b378-d7e17d030a6f
    jinja: "\u0628\u06CC\u0646 \"{{answer_choices[0]}}\" \u0648 \"{{answer_choices[1]}}\"\
      \ \u0631\u0627 \u0627\u0646\u062A\u062E\u0627\u0628 \u06A9\u0646\u06CC\u062F\
      .\n\u0633\u0648\u0627\u0644: {{question}}\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: choose_between
    reference: ''
  92013fab-5387-44d4-bf0f-e29a31bcafb6: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 92013fab-5387-44d4-bf0f-e29a31bcafb6
    jinja: "\u0645\u0646 \u0645\u0646\u0637\u0642 \u062F\u0627\u0646\u0634 \u0622\u0645\
      \u0648\u0632\u0627\u0646\u0645 \u0631\u0627 \u0622\u0632\u0645\u0627\u06CC\u0634\
      \ \u0645\u06CC \u06A9\u0646\u0645.\n\u067E\u0627\u0633\u062E\u06CC \u06A9\u0647\
      \ \u0628\u0627\u06CC\u062F \u0628\u06CC\u0646 \"{{answer_choices[0]}}\" \u0648\
      \ \"{{answer_choices[1]}}\" \u0627\u0646\u062A\u062E\u0627\u0628 \u06A9\u0646\
      \u0646\u062F \u0686\u06CC\u0633\u062A\u061F\n\u0622\u0632\u0645\u0648\u0646\
      \ \u0645\u0646\u0637\u0642\u06CC: {{question}}\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: testing_students
    reference: ''
