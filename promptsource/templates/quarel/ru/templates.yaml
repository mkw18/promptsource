dataset: quarel
subset: ru
templates:
  5904fd73-b1ee-4f89-b7bc-b0fe8cc07c66: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 5904fd73-b1ee-4f89-b7bc-b0fe8cc07c66
    jinja: "\u0412\u043E\u043F\u0440\u043E\u0441: {{question}}\n\n\u041D\u0435 \u0438\
      \u0441\u043F\u043E\u043B\u044C\u0437\u0443\u0439\u0442\u0435 {{\"A\"}} \u0438\
      \ {{\"B\"}}, \u0447\u0442\u043E\u0431\u044B \u043E\u0442\u0432\u0435\u0442\u0438\
      \u0442\u044C \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441, \u0430 \u0432\
      \u043C\u0435\u0441\u0442\u043E \u044D\u0442\u043E\u0433\u043E \u0432\u044B\u0431\
      \u0435\u0440\u0438\u0442\u0435 \u043C\u0435\u0436\u0434\u0443 \"{{answer_choices[0]}}\"\
      \ \u0438 \"{{answer_choices[1]}}\".\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: do_not_use
    reference: ''
  5b5f9d29-0ad5-4bb9-831a-11fcb115c10d: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 5b5f9d29-0ad5-4bb9-831a-11fcb115c10d
    jinja: "\u0412\u043E\u0442 \u0442\u0435\u0441\u0442 \u043D\u0430 \u043B\u043E\u0433\
      \u0438\u043A\u0443: {{question}}\n\n\u0412\u044B\u0431\u0435\u0440\u0438\u0442\
      \u0435 \u043E\u0442\u0432\u0435\u0442 \u043C\u0435\u0436\u0434\u0443 \"{{answer_choices[0]}}\"\
      \ \u0438 \"{{answer_choices[1]}}\".\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: logic_test
    reference: ''
  63c58389-605a-42b9-85a6-a2586a954a92: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 63c58389-605a-42b9-85a6-a2586a954a92
    jinja: "\u0412\u043E\u0442 \u043A\u043E\u0440\u043E\u0442\u043A\u0430\u044F \u0438\
      \u0441\u0442\u043E\u0440\u0438\u044F: {{question}}.\n\n\u041A\u0430\u043A\u043E\
      \u0439 \u0441\u0430\u043C\u044B\u0439 \u0440\u0430\u0437\u0443\u043C\u043D\u044B\
      \u0439 \u043E\u0442\u0432\u0435\u0442 \u043C\u0435\u0436\u0434\u0443 \"{{answer_choices[0]}}\"\
      \ \u0438 \"{{answer_choices[1]}}\"?\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: heres_a_story
    reference: ''
  73a7adbb-41b1-4b4d-b378-d7e17d030a6f: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 73a7adbb-41b1-4b4d-b378-d7e17d030a6f
    jinja: "\u0412\u044B\u0431\u0435\u0440\u0438\u0442\u0435 \u043C\u0435\u0436\u0434\
      \u0443 \"{{answer_choices[0]}}\" \u0438 \"{{answer_choices[1]}}\".\n\u0412\u043E\
      \u043F\u0440\u043E\u0441: {{question}}\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: choose_between
    reference: ''
  92013fab-5387-44d4-bf0f-e29a31bcafb6: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 92013fab-5387-44d4-bf0f-e29a31bcafb6
    jinja: "\u042F \u043F\u0440\u043E\u0432\u0435\u0440\u044F\u044E \u043B\u043E\u0433\
      \u0438\u043A\u0443 \u0441\u0432\u043E\u0438\u0445 \u0443\u0447\u0435\u043D\u0438\
      \u043A\u043E\u0432.\n\u041A\u0430\u043A\u043E\u0439 \u043E\u0442\u0432\u0435\
      \u0442 \u043E\u043D\u0438 \u0434\u043E\u043B\u0436\u043D\u044B \u0432\u044B\u0431\
      \u0440\u0430\u0442\u044C \u043C\u0435\u0436\u0434\u0443 \"{{answer_choices[0]}}\"\
      \ \u0438 \"{{answer_choices[1]}}\"?\n\u041B\u043E\u0433\u0438\u0447\u0435\u0441\
      \u043A\u0438\u0439 \u0442\u0435\u0441\u0442: {{question}}\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: testing_students
    reference: ''
