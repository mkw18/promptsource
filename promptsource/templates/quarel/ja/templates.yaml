dataset: quarel
subset: ja
templates:
  5904fd73-b1ee-4f89-b7bc-b0fe8cc07c66: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 5904fd73-b1ee-4f89-b7bc-b0fe8cc07c66
    jinja: "\u8CEA\u554F\uFF1A {{question}}\n\n{{\"a\"}}\u304A\u3088\u3073{{{\"b\"\
      }}\u3092\u4F7F\u7528\u3057\u3066\u8CEA\u554F\u306B\u7B54\u3048\u306A\u3044\u3067\
      \u304F\u3060\u3055\u3044\u3002\u4EE3\u308F\u308A\u306B\u3001\"{{answer_choices[0]}}\"\
      \u3068\"{{answer_choices[1]}}\"\u306E\u3044\u305A\u308C\u304B\u3092\u9078\u629E\
      \u3057\u307E\u3059\u3002\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: do_not_use
    reference: ''
  5b5f9d29-0ad5-4bb9-831a-11fcb115c10d: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 5b5f9d29-0ad5-4bb9-831a-11fcb115c10d
    jinja: "\u3053\u308C\u304C\u30ED\u30B8\u30C3\u30AF\u30C6\u30B9\u30C8\u3067\u3059\
      \uFF1A {{question}}\n\n\"{{answer_choices[0]}}\"\u3068\"{{answer_choices[1]}}\"\
      \u306E\u9593\u306E\u7B54\u3048\u3092\u9078\u629E\u3057\u307E\u3059\u3002\n|||\n\
      {{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: logic_test
    reference: ''
  63c58389-605a-42b9-85a6-a2586a954a92: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 63c58389-605a-42b9-85a6-a2586a954a92
    jinja: "\u3053\u308C\u304C\u77ED\u7DE8\u5C0F\u8AAC\u3067\u3059\uFF1A {{question}}.\n\
      \n\"{{answer_choices[0]}}\"\u3068\"{{answer_choices[1]}}\"\u306E\u9593\u306E\
      \u6700\u3082\u5B98\u80FD\u7684\u306A\u7B54\u3048\u306F\u4F55\u3067\u3059\u304B\
      \uFF1F\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: heres_a_story
    reference: ''
  73a7adbb-41b1-4b4d-b378-d7e17d030a6f: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 73a7adbb-41b1-4b4d-b378-d7e17d030a6f
    jinja: "\"{{answer_choices[0]}}\"\u3068\"{{answer_choices[1]}}\"\u3092\u9078\u629E\
      \u3057\u307E\u3059\u3002\n\u8CEA\u554F\uFF1A {{question}}\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: choose_between
    reference: ''
  92013fab-5387-44d4-bf0f-e29a31bcafb6: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 92013fab-5387-44d4-bf0f-e29a31bcafb6
    jinja: "\u751F\u5F92\u306E\u8AD6\u7406\u3092\u30C6\u30B9\u30C8\u3057\u3066\u3044\
      \u307E\u3059\u3002\n{\"{{answer_choices[0]}}\"\u3068\"{{answer_choices[1]}}\"\
      \u306E\u3069\u3061\u3089\u304B\u3092\u9078\u629E\u3059\u3079\u304D\u7B54\u3048\
      \u306F\u4F55\u3067\u3059\u304B\uFF1F\n\u30ED\u30B8\u30C3\u30AF\u30C6\u30B9\u30C8\
      \uFF1A {{question}}\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: testing_students
    reference: ''
