dataset: quarel
subset: zh
templates:
  5904fd73-b1ee-4f89-b7bc-b0fe8cc07c66: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 5904fd73-b1ee-4f89-b7bc-b0fe8cc07c66
    jinja: "\u95EE\u9898\uFF1A {{question}}\n\n\u4E0D\u8981\u4F7F\u7528 {{\"A\"}}\
      \ \u548C {{\"B\"}} \u6765\u56DE\u7B54\u95EE\u9898\uFF0C\u800C\u662F\u5728 \"\
      {{answer_choices[0]}}\" \u548C \"{{answer_choices[1]}}\" \u4E4B\u95F4\u8FDB\u884C\
      \u9009\u62E9\u3002\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: do_not_use
    reference: ''
  5b5f9d29-0ad5-4bb9-831a-11fcb115c10d: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 5b5f9d29-0ad5-4bb9-831a-11fcb115c10d
    jinja: "\u8FD9\u662F\u4E00\u4E2A\u903B\u8F91\u6D4B\u8BD5\uFF1A {{question}}\n\n\
      \u5728 \"{{answer_choices[0]}}\" \u548C \"{{answer_choices[1]}}\" \u4E4B\u95F4\
      \u9009\u62E9\u7B54\u6848\u3002\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: logic_test
    reference: ''
  63c58389-605a-42b9-85a6-a2586a954a92: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 63c58389-605a-42b9-85a6-a2586a954a92
    jinja: "\u8FD9\u662F\u4E00\u4E2A\u7B80\u77ED\u7684\u6545\u4E8B\uFF1A {{question}}.\n\
      \n\"{{answer_choices[0]}}\" \u548C \"{{answer_choices[1]}}\" \u4E4B\u95F4\u6700\
      \u5408\u7406\u7684\u7B54\u6848\u662F\u4EC0\u4E48\uFF1F\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: heres_a_story
    reference: ''
  73a7adbb-41b1-4b4d-b378-d7e17d030a6f: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 73a7adbb-41b1-4b4d-b378-d7e17d030a6f
    jinja: "\u5728 \"{{answer_choices[0]}}\" \u548C \"{{answer_choices[1]}}\" \u4E4B\
      \u95F4\u8FDB\u884C\u9009\u62E9\u3002\n\u95EE\u9898\uFF1A {{question}}\n|||\n\
      {{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: choose_between
    reference: ''
  92013fab-5387-44d4-bf0f-e29a31bcafb6: !Template
    answer_choices: '{{world_literals.world1[0]}} ||| {{world_literals.world2[0]}}'
    id: 92013fab-5387-44d4-bf0f-e29a31bcafb6
    jinja: "\u6211\u6B63\u5728\u6D4B\u8BD5\u6211\u7684\u5B66\u751F\u7684\u903B\u8F91\
      \u3002\n\u4ED6\u4EEC\u5E94\u8BE5\u5728 \"{{answer_choices[0]}}\" \u548C \"{{answer_choices[1]}}\"\
      \ \u4E4B\u95F4\u9009\u62E9\u4EC0\u4E48\u7B54\u6848\uFF1F\n\u903B\u8F91\u6D4B\
      \u8BD5\uFF1A {{question}}\n|||\n{{answer_choices[answer_index]}}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: false
    name: testing_students
    reference: ''
